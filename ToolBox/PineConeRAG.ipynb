{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86933c50-4b30-41ed-8dce-a94f27c60faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "api_key = \"api-key-here\"\n",
    "\n",
    "index_name = \"deepseek-rag\"\n",
    "\n",
    "pc = Pinecone(api_key=api_key)\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7ad916-8229-4e1c-84ef-de9cfb02f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be9c880-0109-4076-bd34-60b2556a0f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def upload_docs_to_pinecone(docs):\n",
    "    vectors = embedder.encode(docs).tolist()\n",
    "    ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "    pinecone_records = [\n",
    "        {\"id\": id_, \"values\": vec, \"metadata\": {\"text\": doc}}\n",
    "        for id_, vec, doc in zip(ids, vectors, docs)\n",
    "    ]\n",
    "\n",
    "    index.upsert(vectors=pinecone_records)\n",
    "    print(f\"Uploaded {len(docs)} documents to Pinecone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb5496d-d336-4429-84a1-c39788f8b9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_docs_to_pinecone([\n",
    "    \"Retrieval-Augmented Generation (RAG) improves LLM output by injecting relevant context.\",\n",
    "    \"DeepSeek Coder can be used locally for fast, context-aware code generation.\",\n",
    "    \"Pinecone enables fast vector search for dense embeddings.\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a15731-abbd-4521-9a02-1b78ba1d4c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_docs(query, top_k=3):\n",
    "    query_vec = embedder.encode(query).tolist()\n",
    "    result = index.query(vector=query_vec, top_k=top_k, include_metadata=True)\n",
    "    return [match[\"metadata\"][\"text\"] for match in result[\"matches\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a206b13-62ae-4bdd-9019-02a1cbf98846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "deepseek_path = r\"C:\\GGUF\\godolike\\deepseek-coder-6.7b-instruct-Q4_K_M-GGUF\\deepseek-coder-6.7b-instruct-q4_k_m.gguf\"\n",
    "\n",
    "DeepSeekCode = Llama(\n",
    "    model_path=deepseek_path,\n",
    "    n_gpu_layers=20,\n",
    "    n_ctx=2048,\n",
    "    n_batch=256,\n",
    "    n_threads=6,\n",
    "    use_mlock=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cdd820-f916-4994-91b7-eaeeb990a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_generate(query):\n",
    "    retrieved_context = \"\\n\\n\".join(retrieve_relevant_docs(query))\n",
    "    prompt = f\"\"\"You are a helpful coding assistant.\n",
    "\n",
    "Use the following context to answer the question.\n",
    "\n",
    "Context:\n",
    "{retrieved_context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    output = DeepSeekCode(prompt, max_tokens=512, stop=[\"</s>\"])\n",
    "    return output[\"choices\"][0][\"text\"].strip()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py310-cuda]",
   "language": "python",
   "name": "conda-env-py310-cuda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
