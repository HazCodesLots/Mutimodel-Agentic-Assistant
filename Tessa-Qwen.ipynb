{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc20ceba-24b0-411b-a085-9eb978670773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import os\n",
    "\n",
    "QwenChat_path = r\"C:\\GGUF\\Qwen\\Qwen1.5-1.8B-Chat-GGUF\\qwen1_5-1_8b-chat-q4_k_m.gguf\"\n",
    "QwenCode_path = r\"C:\\GGUF\\Qwen\\Qwen2.5-Coder-1.5B-Instruct-GGUF\\Qwen2.5-Coder-1.5B-Instruct-Q4_K_M.gguf\"\n",
    "\n",
    "HISTORY_DIR = r\"D:\\Work\\Wraps\\Tessa\\History\"\n",
    "os.makedirs(HISTORY_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c5628e-5cd3-40b0-94dc-9c3d555a6c02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "QwenChat = Llama(\n",
    "    model_path=QwenChat_path,\n",
    "    n_gpu_layers=16,\n",
    "    n_ctx=3072,\n",
    "    n_batch=128,\n",
    "    n_threads=6,\n",
    "    use_mlock=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "022ae406-d830-45b9-90f5-1674f2124c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentContext:\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "        self.latest_input = \"\"\n",
    "        self.latest_output = \"\"\n",
    "        self.image_context = \"\"\n",
    "\n",
    "    def add_message(self, source: str, content: str):\n",
    "        source = source.lower()\n",
    "        self.history.append({\"source\": source, \"content\": content})\n",
    "        \n",
    "        if source == \"user\":\n",
    "            self.latest_input = content\n",
    "        elif source in [\"qwenchat\", \"qwencoder\"]:\n",
    "            self.latest_output = content\n",
    "\n",
    "    def get_conversation(self):\n",
    "        return [msg[\"content\"] for msg in self.history]\n",
    "\n",
    "ctx = AgentContext()\n",
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d23da0c6-4370-4cfc-9eb7-efd106fa51bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(context: AgentContext) -> str:\n",
    "    prompt = \"\"\n",
    "    for message in context.history:\n",
    "        role = message[\"source\"].lower()\n",
    "        if role == \"user\":\n",
    "            prompt += f\"<|user|>\\n{message['content']}\\n\"\n",
    "        else:\n",
    "            prompt += f\"<|assistant|>\\n{message['content']}\\n\"\n",
    "    prompt += \"<|assistant|>\\n\"\n",
    "    return prompt\n",
    "\n",
    "def safe_prompt(model, text: str, max_tokens: int = None):\n",
    "    \"\"\"\n",
    "    Tokenizes, trims, and detokenizes a prompt so it fits within the model's context.\n",
    "    \"\"\"\n",
    "    if max_tokens is None:\n",
    "        max_tokens = model.n_ctx()\n",
    "\n",
    "    tokens = model.tokenize(text.encode(\"utf-8\"))\n",
    "\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[-max_tokens:]\n",
    "\n",
    "    return model.detokenize(tokens).decode(\"utf-8\", errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29e7d7eb-9f2d-46fc-9364-b6b1c2953f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qwenchat_respond(user_input: str, context: AgentContext, document_context: str = None):\n",
    "    context.add_message(\"user\", user_input)\n",
    "\n",
    "    full_prompt = \"\"\n",
    "    if document_context:\n",
    "        full_prompt += f\"Document:\\n{document_context.strip()}\\n\\n\"\n",
    "\n",
    "    full_prompt += build_prompt(context)\n",
    "    trimmed_prompt = safe_prompt(QwenChat, full_prompt, max_tokens=QwenChat.n_ctx() - 1024)\n",
    "\n",
    "    output = QwenChat(\n",
    "        prompt=trimmed_prompt,\n",
    "        max_tokens=1024,\n",
    "        temperature=0.7,\n",
    "        stop=[\"<|user|>\", \"<|system|>\"]\n",
    "    )\n",
    "\n",
    "    result = output[\"choices\"][0][\"text\"].strip() if \"choices\" in output else \"[Error: no response]\"\n",
    "    context.add_message(\"qwenchat\", result)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0312023a-0aed-44c1-90e5-4477e667f8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BlipForConditionalGeneration(\n",
       "  (vision_model): BlipVisionModel(\n",
       "    (embeddings): BlipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (encoder): BlipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BlipEncoderLayer(\n",
       "          (self_attn): BlipAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): BlipMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_decoder): BlipTextLMHeadModel(\n",
       "    (bert): BlipTextModel(\n",
       "      (embeddings): BlipTextEmbeddings(\n",
       "        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): BlipTextEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BlipTextLayer(\n",
       "            (attention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BlipTextIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BlipTextOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BlipTextOnlyMLMHead(\n",
       "      (predictions): BlipTextLMPredictionHead(\n",
       "        (transform): BlipTextPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip-image-captioning-base\",\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "blip_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "468fbac4-87fa-4365-ae22-5206eb20bf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blip_respond(image_path: str, prompt: str = \"\", context: AgentContext = None) -> str:\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = blip_processor(images=image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "    generated_ids = blip_model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "    caption = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "    if context is not None:\n",
    "        context.add_message(\"tool\", caption)\n",
    "\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94758ab0-3910-4c78-87f7-bd2df1a034bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blip_to_qwenchat(image_path: str, blip_prompt: str = \"\", qwen_question: str = \"What do you observe?\", context: AgentContext = None) -> str:\n",
    "    caption = blip_respond(image_path, prompt=blip_prompt, context=None)\n",
    "    if context is not None:\n",
    "        context.image_context = caption\n",
    "    return qwenchat_respond(qwen_question, context, document_context=caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75975eaa-7fbf-4083-a692-df3becd1bddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "import easyocr\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "ocr_reader = easyocr.Reader(['en'])\n",
    "faiss_texts = []\n",
    "dimension = embedding_model.get_sentence_embedding_dimension()\n",
    "faiss_index = faiss.IndexFlatL2(dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d65216f-e99e-49ef-9b0d-a5c8fb5f2421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "\n",
    "def pdf_to_images(pdf_path: str, dpi: int = 300):\n",
    "    return convert_from_path(pdf_path, dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffe2fc54-3531-4d2d-a2c0-9e5219a3a7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_similar_text(query, top_k=1):\n",
    "    if faiss_index.ntotal == 0:\n",
    "        return [\"(No data indexed yet. Please process a PDF first.)\"]\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    D, I = faiss_index.search(np.array(query_embedding, dtype=np.float32), top_k)\n",
    "    return [faiss_texts[i] for i in I[0] if 0 <= i < len(faiss_texts)]\n",
    "\n",
    "def process_pdf_with_ocr(pdf_path: str):\n",
    "    images = pdf_to_images(pdf_path)\n",
    "    all_text = []\n",
    "\n",
    "    for page_num, img in enumerate(images):\n",
    "        text_lines = ocr_reader.readtext(np.array(img), detail=0)\n",
    "        page_text = \" \".join(text_lines).strip()\n",
    "        \n",
    "        if page_text:\n",
    "            embedding = embedding_model.encode([page_text])\n",
    "            faiss_index.add(np.array(embedding, dtype=np.float32))\n",
    "            faiss_texts.append(page_text)\n",
    "            all_text.append((page_num, page_text))\n",
    "\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e151d3f9-bf0f-4c2b-878a-561ac8b03cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pdf_ocr(query: str, top_k: int = 3):\n",
    "    if faiss_index.ntotal == 0:\n",
    "        return [\"(No data indexed yet. Please process a PDF first.)\"]\n",
    "\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    D, I = faiss_index.search(np.array(query_embedding, dtype=np.float32), top_k)\n",
    "    return [faiss_texts[i] for i in I[0] if 0 <= i < len(faiss_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d80bf77-0640-4c7a-bac0-2d511b91aa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_pdf_ocr(pdf_path: str, query: str, context: AgentContext):\n",
    "    process_pdf_with_ocr(pdf_path)\n",
    "    results = query_pdf_ocr(query, top_k=1)\n",
    "    document_context = results[0]\n",
    "    return route_to_model(query, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88481257-566b-434d-bb0d-f9df788adbed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "QwenCode = Llama(\n",
    "    model_path=QwenCode_path,\n",
    "    n_gpu_layers=16,\n",
    "    n_ctx=8192,\n",
    "    n_batch=128,\n",
    "    n_threads=6,\n",
    "    use_mlock=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a453de8-1148-41ca-b5f5-dc57af2af39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qwencoder_respond(user_input: str, context: AgentContext, document_context: str = None):\n",
    "    context.add_message(\"user\", user_input)\n",
    "\n",
    "    full_prompt = \"\"\n",
    "    if document_context:\n",
    "        full_prompt += f\"Document:\\n{document_context.strip()}\\n\\n\"\n",
    "    full_prompt += build_prompt(context)\n",
    "    trimmed_prompt = safe_prompt(QwenCode, full_prompt, max_tokens=QwenCode.n_ctx() - 3200)\n",
    "    output = QwenCode(\n",
    "        prompt=trimmed_prompt,\n",
    "        max_tokens=3200,\n",
    "        temperature=0.2,\n",
    "        stop=[\"<|user|>\", \"<|system|>\", \"###\", \"example\"]\n",
    "\n",
    "    )\n",
    "    result = output[\"choices\"][0][\"text\"].strip() if \"choices\" in output else \"[Error: no response]\"\n",
    "    context.add_message(\"qwencoder\", result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4fd8520-b418-4bee-b18f-e993ab00ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_to_model(prompt: str, context: AgentContext) -> str:\n",
    "    code_keywords = [\n",
    "        \"function\", \"class\", \"python\", \"java\", \"code\", \"script\", \"loop\", \"algorithm\",\n",
    "        \"regex\", \"compile\", \"bug\", \"error\", \"fix\", \"sort\", \"data structure\", \"pandas\",\n",
    "        \"API\", \"decorator\", \"recursion\", \"print\", \"for loop\", \"if statement\",\n",
    "        \"train\", \"model\", \"neural network\", \"transformer\", \"architecture\", \"mlp\",\n",
    "        \"loss\", \"dataset\", \"optimizer\", \"backpropagation\", \"torch\", \"tensorflow\"\n",
    "    ]\n",
    "\n",
    "    prompt_lower = prompt.lower()\n",
    "    is_code_related = any(keyword in prompt_lower for keyword in code_keywords)\n",
    "\n",
    "    retrieved_chunks = retrieve_similar_text(prompt, top_k=1)\n",
    "    document_context = retrieved_chunks[0] if retrieved_chunks else \"\"\n",
    "\n",
    "    if is_code_related:\n",
    "        return qwencoder_respond(prompt, context, document_context=document_context)\n",
    "    else:\n",
    "        return qwenchat_respond(prompt, context, document_context=document_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f56372-cc7a-40d2-bf2a-b001c748bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "\n",
    "HISTORY_DIR = r\"D:\\\\Work\\\\Wraps\\\\Tessa\\\\History\"\n",
    "os.makedirs(HISTORY_DIR, exist_ok=True)\n",
    "\n",
    "ctx = AgentContext()\n",
    "\n",
    "def save_chat():\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    path = os.path.join(HISTORY_DIR, f\"chat_{timestamp}.json\")\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(ctx.history, f, indent=2)\n",
    "    return f\"Chat saved to {path}\"\n",
    "\n",
    "def format_pairs(history):\n",
    "    pairs = []\n",
    "    i = 0\n",
    "    while i < len(history):\n",
    "        if history[i][\"source\"] == \"user\":\n",
    "            user_msg = history[i][\"content\"]\n",
    "            i += 1\n",
    "            if i < len(history) and history[i][\"source\"] in [\"tool\", \"qwenchat\", \"qwencoder\", \"blip\"]:\n",
    "                tool_msg = history[i][\"content\"]\n",
    "                pairs.append([user_msg, tool_msg])\n",
    "                i += 1\n",
    "            else:\n",
    "                pairs.append([user_msg, \"\"])\n",
    "        elif history[i][\"source\"] in [\"tool\", \"qwenchat\", \"qwencoder\", \"blip\"]:\n",
    "            tool_msg = history[i][\"content\"]\n",
    "            pairs.append([None, tool_msg])\n",
    "            i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "    return pairs\n",
    "\n",
    "def load_chat(file):\n",
    "    with open(file.name, 'r', encoding='utf-8') as f:\n",
    "        ctx.history = json.load(f)\n",
    "    return gr.update(value=format_pairs(ctx.history))\n",
    "\n",
    "def chat_query(user_input):\n",
    "    _ = route_to_model(user_input, ctx)\n",
    "    return gr.update(value=format_pairs(ctx.history)), \"\"\n",
    "\n",
    "def handle_pdf(pdf_file):\n",
    "    text = process_pdf_with_ocr(pdf_file.name)\n",
    "    return f\"Processed {len(text)} pages via OCR. Now searchable via questions.\"\n",
    "\n",
    "def handle_image(image_path):\n",
    "    if image_path is None:\n",
    "        return gr.update(value=format_pairs(ctx.history)),\n",
    "    caption = blip_respond(image_path, prompt=\"Describe this image\", context=ctx)\n",
    "    return gr.update(value=format_pairs(ctx.history)), \"Image processed\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.HTML(\"\"\"\n",
    "    <style>\n",
    "        .top-row {\n",
    "            display: flex;\n",
    "            justify-content: space-between;\n",
    "            align-items: center;\n",
    "            width: 100%;\n",
    "        }\n",
    "        .top-row h1 {\n",
    "            margin: 0;\n",
    "            font-size: 1.8em;\n",
    "        }\n",
    "        .top-buttons {\n",
    "            display: flex;\n",
    "            gap: 10px;\n",
    "        }\n",
    "    </style>\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=6):\n",
    "            gr.Markdown(\"## ü§ñ Tessa - Multimodal Assistant\")\n",
    "        with gr.Column(scale=6, min_width=300):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1, min_width=150):\n",
    "                    image_upload = gr.UploadButton(label=\"üñºÔ∏è Upload Image\", file_types=[\".png\", \".jpg\", \".jpeg\"])\n",
    "                with gr.Column(scale=1, min_width=150):\n",
    "                    pdf_upload = gr.UploadButton(label=\"üìÑ Upload PDF\", file_types=[\".pdf\"])\n",
    "                with gr.Column(scale=1, min_width=100):\n",
    "                    load_btn = gr.UploadButton(label=\"üìÇ Load Chat\", file_types=[\".json\"])\n",
    "                with gr.Column(scale=1, min_width=100):\n",
    "                    save_btn = gr.Button(\"üíæ Save Chat\")\n",
    "\n",
    "    with gr.Row():\n",
    "        chatbox = gr.Chatbot(label=\"Tessa\", show_label=False, bubble_full_width=True, value=[])\n",
    "\n",
    "    with gr.Row():\n",
    "        query = gr.Textbox(show_label=False, placeholder=\"Type your message here...\")\n",
    "        send_btn = gr.Button(\"Send\")\n",
    "\n",
    "    with gr.Row():\n",
    "        status_bar = gr.Textbox(label=\"Status\", interactive=False)\n",
    "\n",
    "    def update_chat(user_input):\n",
    "        pairs, status = chat_query(user_input)\n",
    "        return gr.update(value=format_pairs(ctx.history)), \"\"\n",
    "\n",
    "    send_btn.click(fn=chat_query, inputs=query, outputs=[chatbox, query])\n",
    "    query.submit(fn=chat_query, inputs=query, outputs=[chatbox, query])\n",
    "    save_btn.click(fn=save_chat, inputs=[], outputs=status_bar)\n",
    "    load_btn.upload(fn=load_chat, inputs=load_btn, outputs=chatbox)\n",
    "    pdf_upload.upload(fn=handle_pdf, inputs=pdf_upload, outputs=status_bar)\n",
    "    image_upload.upload(fn=handle_image, inputs=image_upload, outputs=[chatbox, status_bar])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e0268e-dbcb-4b62-9f5d-4dbb1253bd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py310-cuda]",
   "language": "python",
   "name": "conda-env-py310-cuda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
