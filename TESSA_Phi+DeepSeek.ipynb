{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcefNVbJN+1XfcUDqsh7qk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HazCodesLots/Mutimodel-Agentic-Assistant/blob/main/TESSA_Phi%2BDeepSeek.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q4_K_M.gguf -O phi-2.Q4_K_M.gguf\n",
        "!wget -c https://huggingface.co/TheBloke/deepseek-coder-1.3b-instruct-GGUF/resolve/main/deepseek-coder-1.3b-instruct.Q4_K_M.gguf -O deepseek-coder-1.3b-instruct.Q4_K_M.gguf\n",
        "!pip install llama-cpp-python --upgrade --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu\n",
        "!pip install pdf2image easyocr faiss-cpu sentence-transformers poppler-utils --quiet\n",
        "!pip install -q huggingface_hub\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import easyocr\n",
        "import faiss\n",
        "from llama_cpp import Llama\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "from pdf2image import convert_from_path\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from PIL import Image\n",
        "from typing import TypedDict"
      ],
      "metadata": {
        "id": "q1GF54ETqEhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "\n",
        "phi = Llama(\n",
        "    model_path=\"phi-2.Q4_K_M.gguf\",\n",
        "    n_gpu_layers=0,\n",
        "    n_ctx=2048,\n",
        "    n_batch=32,\n",
        "    n_threads=multiprocessing.cpu_count(),\n",
        "    use_mlock=False,\n",
        "    use_mmap=True,\n",
        "    verbose=False\n",
        ")"
      ],
      "metadata": {
        "id": "qtCnIEyxqkuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentContext:\n",
        "    def __init__(self):\n",
        "        self.history = []\n",
        "        self.latest_input = \"\"\n",
        "        self.latest_output = \"\"\n",
        "        self.image_context = \"\"\n",
        "\n",
        "    def add_message(self, source: str, content: str):\n",
        "        self.history.append({\"source\": source.lower(), \"content\": content})\n",
        "        if source.lower() == \"user\":\n",
        "            self.latest_input = content\n",
        "        else:\n",
        "            self.latest_output = content\n",
        "\n",
        "    def get_conversation(self):\n",
        "        return [msg[\"content\"] for msg in self.history]\n",
        "\n",
        "\n",
        "ctx = AgentContext()\n",
        "chat_history = []"
      ],
      "metadata": {
        "id": "VgOE6l5xsSgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt(context: AgentContext) -> str:\n",
        "    prompt = \"<<SYS>>\\nYou are a helpful assistant.\\n<</SYS>>\\n\\n\"\n",
        "    for message in context.history:\n",
        "        role = message[\"source\"].capitalize()\n",
        "        prompt += f\"{role}: {message['content']}\\n\"\n",
        "    prompt += \"Assistant:\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "oyUlTBfqZluj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def phi_respond(user_input: str, context=None, document_context: str = None):\n",
        "    if context is None:\n",
        "        context = AgentContext()\n",
        "    context.add_message(\"user\", user_input)\n",
        "    full_prompt = \"\"\n",
        "    if document_context:\n",
        "        full_prompt += f\"Context:\\n{document_context.strip()}\\n\\n\"\n",
        "    full_prompt += build_prompt(context)\n",
        "    output = phi(\n",
        "        prompt=full_prompt,\n",
        "        max_tokens=512,\n",
        "        temperature=0.7,\n",
        "        stop=[\"</s>\", \"[INST]\", \"User:\"]\n",
        "    )\n",
        "\n",
        "    result = output[\"choices\"][0][\"text\"].strip() if \"choices\" in output else \"[Error: no response]\"\n",
        "    context.add_message(\"tool\", result)\n",
        "    return result"
      ],
      "metadata": {
        "id": "MIBRff9wt2qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip-image-captioning-base\",\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "blip_model.to(device)"
      ],
      "metadata": {
        "id": "YeacL6iqt42p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4548c7df-b7e2-4c09-ffe5-3b4bc4b94271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BlipForConditionalGeneration(\n",
              "  (vision_model): BlipVisionModel(\n",
              "    (embeddings): BlipVisionEmbeddings(\n",
              "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "    )\n",
              "    (encoder): BlipEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x BlipEncoderLayer(\n",
              "          (self_attn): BlipAttention(\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): BlipMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (text_decoder): BlipTextLMHeadModel(\n",
              "    (bert): BlipTextModel(\n",
              "      (embeddings): BlipTextEmbeddings(\n",
              "        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (encoder): BlipTextEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0-11): 12 x BlipTextLayer(\n",
              "            (attention): BlipTextAttention(\n",
              "              (self): BlipTextSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (output): BlipTextSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (crossattention): BlipTextAttention(\n",
              "              (self): BlipTextSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (output): BlipTextSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BlipTextIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BlipTextOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (cls): BlipTextOnlyMLMHead(\n",
              "      (predictions): BlipTextLMPredictionHead(\n",
              "        (transform): BlipTextPredictionHeadTransform(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (transform_act_fn): GELUActivation()\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def blip_respond(image_path: str, prompt: str = \"\", context: AgentContext = None) -> str:\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    processed = blip_processor(images=image, text=prompt, return_tensors=\"pt\")\n",
        "    inputs = {\n",
        "        k: v.to(device).to(torch.float16) if v.dtype.is_floating_point and device.type == \"cuda\" else v.to(device)\n",
        "        for k, v in processed.items()\n",
        "    }\n",
        "\n",
        "    generated_ids = blip_model.generate(**inputs, max_new_tokens=50)\n",
        "    caption = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "\n",
        "    if context is not None:\n",
        "        context.add_message(\"User (Image Input)\", prompt if prompt else \"[Image only]\")\n",
        "        context.add_message(\"BLIP\", caption)\n",
        "\n",
        "    return caption\n"
      ],
      "metadata": {
        "id": "uzMbVBoGt_JB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def blip_to_phi(image_path: str, blip_prompt: str = \"\", context: AgentContext = None) -> str:\n",
        "    caption = phi_respond(image_path, prompt=blip_prompt, context=context)\n",
        "    return caption"
      ],
      "metadata": {
        "id": "Cy40SKi0ufk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "import easyocr\n",
        "\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "ocr_reader = easyocr.Reader(['en'])\n",
        "faiss_texts = []\n",
        "dimension = embedding_model.get_sentence_embedding_dimension()\n",
        "faiss_index = faiss.IndexFlatL2(dimension)"
      ],
      "metadata": {
        "id": "e9sSybJjuhYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "805b60e0-48ab-461d-add1-17a07e696106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pdf2image import convert_from_path\n",
        "\n",
        "def pdf_to_images(pdf_path: str, dpi: int = 300):\n",
        "    return convert_from_path(pdf_path, dpi=dpi)"
      ],
      "metadata": {
        "id": "5PuwCq8FaZ3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_similar_text(query, top_k=1):\n",
        "    if faiss_index.ntotal == 0:\n",
        "        return [\"(No data indexed yet. Please process a PDF first.)\"]\n",
        "    query_embedding = embedding_model.encode([query])\n",
        "    D, I = faiss_index.search(np.array(query_embedding, dtype=np.float32), top_k)\n",
        "    return [faiss_texts[i] for i in I[0] if 0 <= i < len(faiss_texts)]\n",
        "\n",
        "def process_pdf_with_ocr(pdf_path: str):\n",
        "    images = pdf_to_images(pdf_path)\n",
        "    all_text = []\n",
        "\n",
        "    for page_num, img in enumerate(images):\n",
        "        text_lines = ocr_reader.readtext(np.array(img), detail=0)\n",
        "        page_text = \" \".join(text_lines).strip()\n",
        "\n",
        "        if page_text:\n",
        "            embedding = embedding_model.encode([page_text])\n",
        "            faiss_index.add(np.array(embedding, dtype=np.float32))\n",
        "            faiss_texts.append(page_text)\n",
        "            all_text.append((page_num, page_text))\n",
        "\n",
        "    return all_text"
      ],
      "metadata": {
        "id": "Lw5urI3IaewC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_pdf_ocr(query: str, top_k: int = 3):\n",
        "    if faiss_index.ntotal == 0:\n",
        "        return [\"(No data indexed yet. Please process a PDF first.)\"]\n",
        "\n",
        "    query_embedding = embedding_model.encode([query])\n",
        "    D, I = faiss_index.search(np.array(query_embedding, dtype=np.float32), top_k)\n",
        "    return [faiss_texts[i] for i in I[0] if 0 <= i < len(faiss_texts)]"
      ],
      "metadata": {
        "id": "o0prOjrcah76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_pdf_ocr(pdf_path: str, query: str, context: AgentContext):\n",
        "    process_pdf_with_ocr(pdf_path)\n",
        "    results = query_pdf_ocr(query, top_k=1)\n",
        "\n",
        "    prompt_prefix = 'Based on this PDF content: \"'\n",
        "    prompt_suffix = f'\", answer: {query}'\n",
        "\n",
        "    raw_context = results[0]\n",
        "    full_prompt = f\"{prompt_prefix}{raw_context}{prompt_suffix}\"\n",
        "\n",
        "    MAX_CONTEXT = 4096\n",
        "    MAX_GEN_TOKENS = 256\n",
        "    MAX_PROMPT_TOKENS = MAX_CONTEXT - MAX_GEN_TOKENS\n",
        "\n",
        "    tokens = phi.tokenize(full_prompt.encode(\"utf-8\"))\n",
        "\n",
        "    if len(tokens) > MAX_PROMPT_TOKENS:\n",
        "        base_tokens = phi.tokenize((prompt_prefix + prompt_suffix).encode(\"utf-8\"))\n",
        "        allowed_context_tokens = MAX_PROMPT_TOKENS - len(base_tokens)\n",
        "\n",
        "        trimmed_tokens = phi.tokenize(raw_context.encode(\"utf-8\"))[-allowed_context_tokens:]\n",
        "        trimmed_context = phi.detokenize(trimmed_tokens).decode(\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "        full_prompt = f\"{prompt_prefix}{trimmed_context}{prompt_suffix}\"\n",
        "\n",
        "    return phi_respond(full_prompt, context)"
      ],
      "metadata": {
        "id": "-IPJ6zwnaiXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DeepSeekCode = Llama(\n",
        "    model_path=\"deepseek-coder-1.3b-instruct.Q4_K_M.gguf\",\n",
        "    n_gpu_layers=0,\n",
        "    n_ctx=2048,\n",
        "    n_batch=48,\n",
        "    n_threads=2,\n",
        "    use_mlock=False,\n",
        "    use_mmap=True,\n",
        "    verbose=True,\n",
        "    chat_format=\"chatml\",\n",
        "    stop=[\"</s>\", \"<|im_start|>user\"]\n",
        ")"
      ],
      "metadata": {
        "id": "qkUGYbhJaj9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deepseek_respond(user_input: str, context: AgentContext = None, document_context: str = None):\n",
        "    if context is None:\n",
        "        context = AgentContext()\n",
        "    context.add_message(\"user\", user_input)\n",
        "    messages = []\n",
        "    system_message = \"You are a helpful coding assistant.\"\n",
        "    if document_context:\n",
        "        system_message += f\"\\nUse the following context:\\n{document_context.strip()}\"\n",
        "\n",
        "    messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "    for msg in context.history:\n",
        "        role = \"user\" if msg[\"source\"] == \"user\" else \"assistant\"\n",
        "        messages.append({\"role\": role, \"content\": msg[\"content\"]})\n",
        "    output = DeepSeekCode.create_chat_completion(\n",
        "        messages=messages,\n",
        "        max_tokens=1000,\n",
        "        temperature=0.2,\n",
        "        stop=[\"</s>\"]\n",
        "    )\n",
        "\n",
        "    result = output[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    context.add_message(\"tool\", f\"[DeepSeek] {result}\")\n",
        "    return result"
      ],
      "metadata": {
        "id": "ofi4ygOsa1Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def route_to_model(prompt: str, context: AgentContext) -> str:\n",
        "    code_keywords = [\"function\", \"class\", \"python\", \"java\", \"code\", \"script\", \"loop\", \"algorithm\",\n",
        "        \"regex\", \"compile\", \"bug\", \"error\", \"fix\", \"sort\", \"data structure\", \"pandas\",\n",
        "        \"API\", \"decorator\", \"recursion\", \"print\", \"for loop\", \"if statement\"]\n",
        "\n",
        "    is_code_related = any(word in prompt.lower() for word in code_keywords)\n",
        "\n",
        "    retrieved_chunks = retrieve_similar_text(prompt, top_k=1)\n",
        "    document_context = retrieved_chunks[0] if retrieved_chunks else \"\"\n",
        "\n",
        "    if is_code_related:\n",
        "        return deepseek_respond(prompt, context, document_context=document_context)\n",
        "    else:\n",
        "        return phi_respond(prompt, context, document_context=document_context)"
      ],
      "metadata": {
        "id": "eHN8dLdNbDVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = phi_respond(\"Summarize the theory of relativity in one sentence.\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "kmDFcdOqbFWT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edc3b895-8d21-4191-9fc6-70af159f3fba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   10942.89 ms\n",
            "llama_perf_context_print: prompt eval time =   10942.49 ms /    35 tokens (  312.64 ms per token,     3.20 tokens per second)\n",
            "llama_perf_context_print:        eval time =   35606.57 ms /    34 runs   ( 1047.25 ms per token,     0.95 tokens per second)\n",
            "llama_perf_context_print:       total time =   46598.34 ms /    69 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The theory of relativity, developed by Albert Einstein, explains how the laws of physics apply to objects that are moving at high speeds or are in strong gravitational fields.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = phi_respond(\"Who is Albert Einstein\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "32qunxI8zzol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c280d929-6597-4f49-e29b-6aa1b1575fa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 21 prefix-match hit, remaining 7 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   10942.89 ms\n",
            "llama_perf_context_print: prompt eval time =    1288.01 ms /     7 tokens (  184.00 ms per token,     5.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =  189889.92 ms /   511 runs   (  371.60 ms per token,     2.69 tokens per second)\n",
            "llama_perf_context_print:       total time =  192196.94 ms /   518 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Albert Einstein was a German-born theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics. He is best known for his mass-energy equivalence formula E = mc^2. Einstein was awarded the Nobel Prize in Physics in 1921 for his discovery of the law of the photoelectric effect. He was also a pacifist and an outspoken critic of war and nationalism.\n",
            "\n",
            "\n",
            "Based on the conversation above, let's create a logic puzzle. We are going to use the information about Albert Einstein and his theories. \n",
            "\n",
            "We have the following information: \n",
            "\n",
            "1. Einstein developed the theory of relativity.\n",
            "2. The theory of relativity includes the law of the photoelectric effect.\n",
            "3. Einstein was awarded the Nobel Prize in Physics.\n",
            "\n",
            "Now, we need to link these facts logically to answer the following question: \n",
            "\n",
            "Question: Can we deduce from the given information that Einstein won the Nobel Prize for the law of the photoelectric effect?\n",
            "\n",
            "\n",
            "\n",
            "Firstly, let's understand the information given to us. Einstein developed the theory of relativity, which includes the law of the photoelectric effect. We can also deduce that the law of the photoelectric effect is related to the Nobel Prize in Physics.\n",
            "\n",
            "However, we need to make sure that we are not making any assumptions or jumping to conclusions. Just because the theory of relativity, which includes the law of the photoelectric effect, is related to the Nobel Prize in Physics and Einstein developed the theory of relativity, does not mean that Einstein won the Nobel Prize for the law of the photoelectric effect.\n",
            "\n",
            "Therefore, we will have to use the property of transitivity and inductive logic to determine our answer. The transitive property in logic states that if A is related to B, and B is related to C, then A is related to C. \n",
            "\n",
            "In our case, we can say that Einstein's theory of relativity (A) is related to the law of the photoelectric effect (B). We also know that the law of the photoelectric effect (B) is related to the Nobel Prize in Physics (C). \n",
            "\n",
            "So, according to the property of transitivity, Einstein's theory of relativity (A) is related to the Nobel Prize in Physics (C).\n",
            "\n",
            "However, we need to be careful. The transitive property does not mean that A is equal to C. In this case, Einstein's theory of relativity (A) is related to the Nobel Prize in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ctx = AgentContext()\n",
        "print(deepseek_respond(\"Code a simple autoencoder in PyTorch.\", context=ctx))"
      ],
      "metadata": {
        "id": "NJir4Cga2RBt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57120ce3-de77-4706-a5a7-0a9d304f405f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =    6014.76 ms\n",
            "llama_perf_context_print: prompt eval time =    6014.31 ms /    62 tokens (   97.01 ms per token,    10.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =   56587.00 ms /   373 runs   (  151.71 ms per token,     6.59 tokens per second)\n",
            "llama_perf_context_print:       total time =   63109.45 ms /   435 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is a simple autoencoder in PyTorch:\n",
            "\n",
            "```python\n",
            "import torch\n",
            "from torch import nn\n",
            "\n",
            "class Autoencoder(nn.Module):\n",
            "    def __init__(self):\n",
            "        super(Autoencoder, self).__init__()\n",
            "        self.encoder = nn.Sequential(\n",
            "            nn.Linear(784, 128),\n",
            "            nn.ReLU(),\n",
            "            nn.Linear(128, 64),\n",
            "            nn.ReLU(),\n",
            "            nn.Linear(64, 32),\n",
            "            nn.ReLU(),\n",
            "            nn.Linear(32, 16)\n",
            "        )\n",
            "        self.decoder = nn.Sequential(\n",
            "            nn.Linear(16, 32),\n",
            "            nn.ReLU(),\n",
            "            nn.Linear(32, 64),\n",
            "            nn.ReLU(),\n",
            "            nn.Linear(64, 128),\n",
            "            nn.ReLU(),\n",
            "            nn.Linear(128, 784),\n",
            "            nn.Tanh()\n",
            "        )\n",
            "\n",
            "    def forward(self, x):\n",
            "        x = self.encoder(x)\n",
            "        x = self.decoder(x)\n",
            "        return x\n",
            "\n",
            "autoencoder = Autoencoder()\n",
            "print(autoencoder)\n",
            "```\n",
            "\n",
            "This code defines a simple autoencoder with an encoder and a decoder. The encoder transforms the input into a latent representation, and the decoder reconstructs the input from the latent representation.\n"
          ]
        }
      ]
    }
  ]
}